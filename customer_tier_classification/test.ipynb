{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "374b5d5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Loading: open/train.csv\n",
      "Numeric cols (6): ['age', 'tenure', 'frequent', 'payment_interval', 'contract_length', 'after_interaction']\n",
      "Categorical cols (2): ['gender', 'subscription_type']\n",
      "Models considered: ['logreg', 'rf', 'gbdt']\n",
      "Saved CV results -> open/ml_outputs/cv_results.csv\n",
      "Best model: gbdt\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "multiclass format is not supported",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 459\u001b[39m\n\u001b[32m    455\u001b[39m         log(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNOTE: Positive rate is low (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpos_rate\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m). Consider alternative thresholds or cost-sensitive objectives.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    458\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m459\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 394\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    391\u001b[39m log(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mBest model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    393\u001b[39m \u001b[38;5;66;03m# Threshold tuning on OOF predictions\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m394\u001b[39m thr, best_f1_oof, oof_proba = \u001b[43moof_threshold_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpre\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbest_clf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_folds\u001b[49m\u001b[43m=\u001b[49m\u001b[43mN_FOLDS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    395\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(OUTPUT_DIR / \u001b[33m\"\u001b[39m\u001b[33mthreshold_report.txt\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mw\u001b[39m\u001b[33m\"\u001b[39m, encoding=\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m    396\u001b[39m     f.write(json.dumps({\n\u001b[32m    397\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33moof_best_threshold\u001b[39m\u001b[33m\"\u001b[39m: thr,\n\u001b[32m    398\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33moof_best_f1\u001b[39m\u001b[33m\"\u001b[39m: best_f1_oof,\n\u001b[32m    399\u001b[39m     }, indent=\u001b[32m2\u001b[39m))\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 259\u001b[39m, in \u001b[36moof_threshold_search\u001b[39m\u001b[34m(pre, clf, X_train, y_train, n_folds)\u001b[39m\n\u001b[32m    257\u001b[39m pipe = Pipeline(steps=[(\u001b[33m\"\u001b[39m\u001b[33mpre\u001b[39m\u001b[33m\"\u001b[39m, pre), (\u001b[33m\"\u001b[39m\u001b[33mclf\u001b[39m\u001b[33m\"\u001b[39m, clf)])\n\u001b[32m    258\u001b[39m oof_proba = cross_val_predict(pipe, X_train, y_train, cv=cv, method=\u001b[33m\"\u001b[39m\u001b[33mpredict_proba\u001b[39m\u001b[33m\"\u001b[39m, n_jobs=-\u001b[32m1\u001b[39m)[:, \u001b[32m1\u001b[39m]\n\u001b[32m--> \u001b[39m\u001b[32m259\u001b[39m precision, recall, thresh = \u001b[43mprecision_recall_curve\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moof_proba\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    260\u001b[39m f1_vals = (\u001b[32m2\u001b[39m * precision * recall) / (precision + recall + \u001b[32m1e-12\u001b[39m)\n\u001b[32m    261\u001b[39m best_idx = \u001b[38;5;28mint\u001b[39m(np.nanargmax(f1_vals))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git/python-playground/.venv/lib/python3.12/site-packages/sklearn/utils/_param_validation.py:218\u001b[39m, in \u001b[36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    212\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    213\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m    214\u001b[39m         skip_parameter_validation=(\n\u001b[32m    215\u001b[39m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m    216\u001b[39m         )\n\u001b[32m    217\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m218\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    220\u001b[39m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[32m    224\u001b[39m     msg = re.sub(\n\u001b[32m    225\u001b[39m         \u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[33m\\\u001b[39m\u001b[33mw+ must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    226\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc.\u001b[34m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    227\u001b[39m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[32m    228\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git/python-playground/.venv/lib/python3.12/site-packages/sklearn/metrics/_ranking.py:1018\u001b[39m, in \u001b[36mprecision_recall_curve\u001b[39m\u001b[34m(y_true, y_score, pos_label, sample_weight, drop_intermediate)\u001b[39m\n\u001b[32m    911\u001b[39m \u001b[38;5;129m@validate_params\u001b[39m(\n\u001b[32m    912\u001b[39m     {\n\u001b[32m    913\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33my_true\u001b[39m\u001b[33m\"\u001b[39m: [\u001b[33m\"\u001b[39m\u001b[33marray-like\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m   (...)\u001b[39m\u001b[32m    927\u001b[39m     drop_intermediate=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    928\u001b[39m ):\n\u001b[32m    929\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Compute precision-recall pairs for different probability thresholds.\u001b[39;00m\n\u001b[32m    930\u001b[39m \n\u001b[32m    931\u001b[39m \u001b[33;03m    Note: this implementation is restricted to the binary classification task.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1016\u001b[39m \u001b[33;03m    array([0.1 , 0.35, 0.4 , 0.8 ])\u001b[39;00m\n\u001b[32m   1017\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1018\u001b[39m     fps, tps, thresholds = \u001b[43m_binary_clf_curve\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1019\u001b[39m \u001b[43m        \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_score\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos_label\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpos_label\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[43msample_weight\u001b[49m\n\u001b[32m   1020\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1022\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m drop_intermediate \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(fps) > \u001b[32m2\u001b[39m:\n\u001b[32m   1023\u001b[39m         \u001b[38;5;66;03m# Drop thresholds corresponding to points where true positives (tps)\u001b[39;00m\n\u001b[32m   1024\u001b[39m         \u001b[38;5;66;03m# do not change from the previous or subsequent point. This will keep\u001b[39;00m\n\u001b[32m   1025\u001b[39m         \u001b[38;5;66;03m# only the first and last point for each tps value. All points\u001b[39;00m\n\u001b[32m   1026\u001b[39m         \u001b[38;5;66;03m# with the same tps value have the same recall and thus x coordinate.\u001b[39;00m\n\u001b[32m   1027\u001b[39m         \u001b[38;5;66;03m# They appear as a vertical line on the plot.\u001b[39;00m\n\u001b[32m   1028\u001b[39m         optimal_idxs = np.where(\n\u001b[32m   1029\u001b[39m             np.concatenate(\n\u001b[32m   1030\u001b[39m                 [[\u001b[38;5;28;01mTrue\u001b[39;00m], np.logical_or(np.diff(tps[:-\u001b[32m1\u001b[39m]), np.diff(tps[\u001b[32m1\u001b[39m:])), [\u001b[38;5;28;01mTrue\u001b[39;00m]]\n\u001b[32m   1031\u001b[39m             )\n\u001b[32m   1032\u001b[39m         )[\u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git/python-playground/.venv/lib/python3.12/site-packages/sklearn/metrics/_ranking.py:863\u001b[39m, in \u001b[36m_binary_clf_curve\u001b[39m\u001b[34m(y_true, y_score, pos_label, sample_weight)\u001b[39m\n\u001b[32m    861\u001b[39m y_type = type_of_target(y_true, input_name=\u001b[33m\"\u001b[39m\u001b[33my_true\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    862\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (y_type == \u001b[33m\"\u001b[39m\u001b[33mbinary\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m (y_type == \u001b[33m\"\u001b[39m\u001b[33mmulticlass\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m pos_label \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)):\n\u001b[32m--> \u001b[39m\u001b[32m863\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[33m format is not supported\u001b[39m\u001b[33m\"\u001b[39m.format(y_type))\n\u001b[32m    865\u001b[39m check_consistent_length(y_true, y_score, sample_weight)\n\u001b[32m    866\u001b[39m y_true = column_or_1d(y_true)\n",
      "\u001b[31mValueError\u001b[39m: multiclass format is not supported"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Support Needs Prediction (Binary) — No AutoML\n",
    "================================================\n",
    "- Input columns (example):\n",
    "  ID, age, gender, tenure, frequent, payment_interval, subscription_type, contract_length, after_interaction, support_needs\n",
    "- Goal: Predict `support_needs` (0/1) without AutoML frameworks.\n",
    "- What this script does:\n",
    "  1) Load & split data (stratified train/test)\n",
    "  2) Preprocess (impute, scale numeric; impute + one-hot categorical)\n",
    "  3) Train several fixed-parameter models (no hyperparameter search)\n",
    "  4) Cross-validate on train and pick the best model by PR-AUC (average_precision)\n",
    "  5) Tune decision threshold on out-of-fold predictions to maximize F1\n",
    "  6) Evaluate on test (ROC-AUC, PR-AUC, F1, precision, recall, confusion matrix)\n",
    "  7) Compute permutation feature importance and export artifacts\n",
    "  8) Save the fitted pipeline to disk (joblib)\n",
    "\n",
    "- Outputs (created under ./ml_outputs/):\n",
    "  - cv_results.csv, best_model.txt\n",
    "  - threshold_report.txt\n",
    "  - test_metrics.txt, confusion_matrix.png\n",
    "  - permutation_importance.csv, permutation_importance.png\n",
    "  - predictions_test.csv (ID, y_true, y_prob, y_pred)\n",
    "  - support_needs_model.pkl (joblib pipeline)\n",
    "\n",
    "Notes\n",
    "-----\n",
    "- No AutoML libraries (e.g., autosklearn/TPOT) used; only fixed configurations and simple model selection.\n",
    "- If class imbalance is strong, class_weight/scale_pos_weight are set.\n",
    "- XGBoost/LightGBM are optional (used only if installed). Otherwise skip.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_validate, cross_val_predict\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score,\n",
    "    average_precision_score,\n",
    "    f1_score,\n",
    "    precision_recall_curve,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    confusion_matrix,\n",
    "    classification_report,\n",
    ")\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "try:\n",
    "    import joblib\n",
    "except Exception:\n",
    "    joblib = None\n",
    "\n",
    "# =============================\n",
    "# Config\n",
    "# =============================\n",
    "CSV_PATH = Path(\"./open/train.csv\")\n",
    "TARGET_COL = \"support_needs\"\n",
    "ID_COL = \"ID\"\n",
    "RANDOM_STATE = 42\n",
    "TEST_SIZE = 0.2\n",
    "N_FOLDS = 5\n",
    "TOP_K_FRAC = 0.1  # Evaluate recall within top k% highest-probability cases\n",
    "OUTPUT_DIR = CSV_PATH.parent / \"ml_outputs\"\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Columns expected (numeric/cat). We'll infer but can be overridden here.\n",
    "NUM_HINT = [\n",
    "    \"age\", \"tenure\", \"frequent\", \"payment_interval\",\n",
    "    \"contract_length\", \"after_interaction\"\n",
    "]\n",
    "CAT_HINT = [\"gender\", \"subscription_type\"]\n",
    "\n",
    "# =============================\n",
    "# Utility\n",
    "# =============================\n",
    "\n",
    "def log(msg: str):\n",
    "    print(msg)\n",
    "\n",
    "\n",
    "def read_data(csv_path: str) -> pd.DataFrame:\n",
    "    log(f\">> Loading: {csv_path}\")\n",
    "    df = pd.read_csv(csv_path)\n",
    "    return df\n",
    "\n",
    "\n",
    "def split_xy(df: pd.DataFrame):\n",
    "    assert TARGET_COL in df.columns, f\"Target column '{TARGET_COL}' not found.\"\n",
    "    y = pd.to_numeric(df[TARGET_COL], errors=\"coerce\").fillna(0).astype(int)\n",
    "    X = df.drop(columns=[TARGET_COL])\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def infer_column_types(df: pd.DataFrame):\n",
    "    cols = df.columns.tolist()\n",
    "    cat_cols = []\n",
    "    num_cols = []\n",
    "    for c in cols:\n",
    "        if c == TARGET_COL:\n",
    "            continue\n",
    "        if c == ID_COL:\n",
    "            continue\n",
    "        if c in NUM_HINT:\n",
    "            num_cols.append(c)\n",
    "        elif c in CAT_HINT:\n",
    "            cat_cols.append(c)\n",
    "        else:\n",
    "            # heuristic: numeric-looking -> numeric else categorical\n",
    "            if pd.api.types.is_numeric_dtype(df[c]):\n",
    "                num_cols.append(c)\n",
    "            else:\n",
    "                # Try coercion\n",
    "                sample = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "                if sample.notna().mean() > 0.9:\n",
    "                    num_cols.append(c)\n",
    "                else:\n",
    "                    cat_cols.append(c)\n",
    "    return num_cols, cat_cols\n",
    "\n",
    "\n",
    "def build_preprocessor(num_cols, cat_cols):\n",
    "    num_pipe = Pipeline(\n",
    "        steps=[\n",
    "            (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "            (\"scaler\", StandardScaler()),\n",
    "        ]\n",
    "    )\n",
    "    cat_pipe = Pipeline(\n",
    "        steps=[\n",
    "            (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "            (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\")),\n",
    "        ]\n",
    "    )\n",
    "    pre = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"num\", num_pipe, num_cols),\n",
    "            (\"cat\", cat_pipe, cat_cols),\n",
    "        ],\n",
    "        remainder=\"drop\",\n",
    "        verbose_feature_names_out=False,\n",
    "    )\n",
    "    return pre\n",
    "\n",
    "\n",
    "def get_models(y_train: pd.Series):\n",
    "    \"\"\"Return dict of fixed-parameter models (no HPO). Uses class weights for imbalance.\"\"\"\n",
    "    pos_rate = y_train.mean()\n",
    "    neg_rate = 1 - pos_rate\n",
    "    scale_pos_weight = (neg_rate / pos_rate) if pos_rate > 0 else 1.0\n",
    "\n",
    "    models = {\n",
    "        \"logreg\": LogisticRegression(\n",
    "            max_iter=2000,\n",
    "            class_weight=\"balanced\",\n",
    "            n_jobs=None if hasattr(LogisticRegression, 'n_jobs') else None,\n",
    "        ),\n",
    "        \"rf\": RandomForestClassifier(\n",
    "            n_estimators=400,\n",
    "            max_depth=None,\n",
    "            min_samples_split=2,\n",
    "            min_samples_leaf=1,\n",
    "            class_weight=\"balanced_subsample\",\n",
    "            random_state=RANDOM_STATE,\n",
    "            n_jobs=-1,\n",
    "        ),\n",
    "        \"gbdt\": GradientBoostingClassifier(\n",
    "            random_state=RANDOM_STATE,\n",
    "        ),\n",
    "    }\n",
    "\n",
    "    # Optional: XGBoost\n",
    "    try:\n",
    "        import xgboost as xgb\n",
    "        models[\"xgb\"] = xgb.XGBClassifier(\n",
    "            n_estimators=500,\n",
    "            max_depth=5,\n",
    "            learning_rate=0.05,\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.8,\n",
    "            reg_lambda=1.0,\n",
    "            random_state=RANDOM_STATE,\n",
    "            tree_method=\"hist\",\n",
    "            eval_metric=\"logloss\",\n",
    "            n_jobs=-1,\n",
    "            scale_pos_weight=scale_pos_weight,\n",
    "        )\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # Optional: LightGBM\n",
    "    try:\n",
    "        import lightgbm as lgb\n",
    "        models[\"lgbm\"] = lgb.LGBMClassifier(\n",
    "            n_estimators=800,\n",
    "            num_leaves=63,\n",
    "            learning_rate=0.03,\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.8,\n",
    "            reg_lambda=1.0,\n",
    "            random_state=RANDOM_STATE,\n",
    "            class_weight=\"balanced\",\n",
    "            n_jobs=-1,\n",
    "        )\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    return models\n",
    "\n",
    "\n",
    "def cross_validate_models(pre, models: dict, X_train: pd.DataFrame, y_train: pd.Series, n_folds: int = 5) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    cv = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=RANDOM_STATE)\n",
    "    scoring = {\n",
    "        \"roc_auc\": \"roc_auc\",\n",
    "        \"average_precision\": \"average_precision\",  # PR-AUC\n",
    "        \"f1\": \"f1\",\n",
    "        \"precision\": \"precision\",\n",
    "        \"recall\": \"recall\",\n",
    "    }\n",
    "    for name, clf in models.items():\n",
    "        pipe = Pipeline(steps=[(\"pre\", pre), (\"clf\", clf)])\n",
    "        scores = cross_validate(pipe, X_train, y_train, cv=cv, scoring=scoring, n_jobs=-1, return_train_score=False)\n",
    "        rows.append({\n",
    "            \"model\": name,\n",
    "            **{f\"mean_{k}\": np.mean(v) for k, v in scores.items() if k.startswith(\"test_\")},\n",
    "            **{f\"std_{k}\": np.std(v) for k, v in scores.items() if k.startswith(\"test_\")},\n",
    "        })\n",
    "    df = pd.DataFrame(rows)\n",
    "    order = df.sort_values(\"mean_test_average_precision\", ascending=False)\n",
    "    return order\n",
    "\n",
    "\n",
    "def pick_best_model(cv_df: pd.DataFrame) -> str:\n",
    "    # Select by highest PR-AUC (average_precision)\n",
    "    best = cv_df.iloc[0]\n",
    "    return best[\"model\"]\n",
    "\n",
    "\n",
    "def oof_threshold_search(pre, clf, X_train, y_train, n_folds: int = 5):\n",
    "    \"\"\"Get OOF probabilities and choose threshold that maximizes F1.\"\"\"\n",
    "    cv = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=RANDOM_STATE)\n",
    "    pipe = Pipeline(steps=[(\"pre\", pre), (\"clf\", clf)])\n",
    "    oof_proba = cross_val_predict(pipe, X_train, y_train, cv=cv, method=\"predict_proba\", n_jobs=-1)[:, 1]\n",
    "    precision, recall, thresh = precision_recall_curve(y_train, oof_proba)\n",
    "    f1_vals = (2 * precision * recall) / (precision + recall + 1e-12)\n",
    "    best_idx = int(np.nanargmax(f1_vals))\n",
    "    best_threshold = 0.5 if best_idx >= len(thresh) else float(thresh[best_idx])\n",
    "    best_f1 = float(np.nanmax(f1_vals))\n",
    "    return best_threshold, best_f1, oof_proba\n",
    "\n",
    "\n",
    "def evaluate_on_test(pre, clf, X_train, y_train, X_test, y_test, threshold: float):\n",
    "    pipe = Pipeline(steps=[(\"pre\", pre), (\"clf\", clf)])\n",
    "    pipe.fit(X_train, y_train)\n",
    "    proba = pipe.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    # Metrics\n",
    "    roc = roc_auc_score(y_test, proba)\n",
    "    ap = average_precision_score(y_test, proba)\n",
    "    y_pred = (proba >= threshold).astype(int)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    prec = precision_score(y_test, y_pred, zero_division=0)\n",
    "    rec = recall_score(y_test, y_pred)\n",
    "\n",
    "    # Top-K recall (capture rate in top-k%)\n",
    "    k = int(np.ceil(len(proba) * TOP_K_FRAC))\n",
    "    top_idx = np.argsort(-proba)[:k]\n",
    "    topk_recall = y_test.iloc[top_idx].sum() / y_test.sum() if y_test.sum() > 0 else 0.0\n",
    "\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    report = classification_report(y_test, y_pred, digits=4)\n",
    "\n",
    "    return {\n",
    "        \"pipeline\": pipe,\n",
    "        \"proba\": proba,\n",
    "        \"y_pred\": y_pred,\n",
    "        \"roc_auc\": roc,\n",
    "        \"pr_auc\": ap,\n",
    "        \"f1\": f1,\n",
    "        \"precision\": prec,\n",
    "        \"recall\": rec,\n",
    "        \"topk_recall\": float(topk_recall),\n",
    "        \"confusion_matrix\": cm,\n",
    "        \"cls_report\": report,\n",
    "    }\n",
    "\n",
    "\n",
    "def get_feature_names(pre, num_cols, cat_cols):\n",
    "    names = []\n",
    "    if len(num_cols) > 0:\n",
    "        names.extend(num_cols)\n",
    "    if len(cat_cols) > 0:\n",
    "        ohe = pre.named_transformers_[\"cat\"].named_steps[\"ohe\"]\n",
    "        try:\n",
    "            ohe_names = ohe.get_feature_names_out(cat_cols).tolist()\n",
    "        except Exception:\n",
    "            # Fallback for older scikit-learn\n",
    "            ohe_names = []\n",
    "            for i, c in enumerate(cat_cols):\n",
    "                cats = ohe.categories_[i]\n",
    "                ohe_names.extend([f\"{c}_{val}\" for val in cats])\n",
    "        names.extend(ohe_names)\n",
    "    return names\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(cm, outpath: Path):\n",
    "    fig, ax = plt.subplots(figsize=(4,4))\n",
    "    im = ax.imshow(cm, cmap=\"Blues\")\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    ax.set(xticks=[0,1], yticks=[0,1], xticklabels=[0,1], yticklabels=[0,1],\n",
    "           ylabel='True label', xlabel='Predicted label', title='Confusion Matrix')\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], 'd'), ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(outpath, dpi=140)\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "def plot_permutation_importance(pipe, X_test, y_test, feature_names, out_csv: Path, out_png: Path):\n",
    "    r = permutation_importance(pipe, X_test, y_test, n_repeats=10, random_state=RANDOM_STATE, n_jobs=-1, scoring=\"average_precision\")\n",
    "    imp = pd.DataFrame({\"feature\": feature_names, \"importance_mean\": r.importances_mean, \"importance_std\": r.importances_std})\n",
    "    imp.sort_values(\"importance_mean\", ascending=False, inplace=True)\n",
    "    imp.to_csv(out_csv, index=False)\n",
    "\n",
    "    top = imp.head(20)\n",
    "    fig, ax = plt.subplots(figsize=(7, 6))\n",
    "    ax.barh(top[\"feature\"][::-1], top[\"importance_mean\"][::-1])\n",
    "    ax.set_title(\"Permutation Importance (PR-AUC)\")\n",
    "    ax.set_xlabel(\"Mean Importance\")\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(out_png, dpi=140)\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "def main():\n",
    "    df = read_data(CSV_PATH)\n",
    "\n",
    "    # Optional: keep ID aside\n",
    "    id_series = None\n",
    "    if ID_COL in df.columns:\n",
    "        id_series = df[ID_COL].astype(str)\n",
    "\n",
    "    X, y = split_xy(df)\n",
    "\n",
    "    # Infer column types\n",
    "    num_cols, cat_cols = infer_column_types(X)\n",
    "    log(f\"Numeric cols ({len(num_cols)}): {num_cols}\")\n",
    "    log(f\"Categorical cols ({len(cat_cols)}): {cat_cols}\")\n",
    "\n",
    "    # Train/test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=TEST_SIZE, stratify=y, random_state=RANDOM_STATE\n",
    "    )\n",
    "\n",
    "    # Preprocessor\n",
    "    pre = build_preprocessor(num_cols, cat_cols)\n",
    "\n",
    "    # Models\n",
    "    models = get_models(y_train)\n",
    "    log(f\"Models considered: {list(models.keys())}\")\n",
    "\n",
    "    # Cross-validate models on train\n",
    "    cv_df = cross_validate_models(pre, models, X_train, y_train, n_folds=N_FOLDS)\n",
    "    cv_path = OUTPUT_DIR / \"cv_results.csv\"\n",
    "    cv_df.to_csv(cv_path, index=False)\n",
    "    log(f\"Saved CV results -> {cv_path}\")\n",
    "\n",
    "    best_name = pick_best_model(cv_df)\n",
    "    best_clf = models[best_name]\n",
    "    with open(OUTPUT_DIR / \"best_model.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(f\"Best by PR-AUC: {best_name}\\n\")\n",
    "        f.write(cv_df.head(1).to_string(index=False))\n",
    "    log(f\"Best model: {best_name}\")\n",
    "\n",
    "    # Threshold tuning on OOF predictions\n",
    "    thr, best_f1_oof, oof_proba = oof_threshold_search(pre, best_clf, X_train, y_train, n_folds=N_FOLDS)\n",
    "    with open(OUTPUT_DIR / \"threshold_report.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(json.dumps({\n",
    "            \"oof_best_threshold\": thr,\n",
    "            \"oof_best_f1\": best_f1_oof,\n",
    "        }, indent=2))\n",
    "    log(f\"Chosen threshold (OOF, max F1): {thr:.4f} | OOF F1: {best_f1_oof:.4f}\")\n",
    "\n",
    "    # Final train on full train, evaluate on test\n",
    "    result = evaluate_on_test(pre, best_clf, X_train, y_train, X_test, y_test, threshold=thr)\n",
    "\n",
    "    # Save metrics\n",
    "    metrics_path = OUTPUT_DIR / \"test_metrics.txt\"\n",
    "    with open(metrics_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\n",
    "            f\"ROC-AUC: {result['roc_auc']:.4f}\\nPR-AUC: {result['pr_auc']:.4f}\\nF1: {result['f1']:.4f}\\n\"\n",
    "            f\"Precision: {result['precision']:.4f}\\nRecall: {result['recall']:.4f}\\nTop{int(TOP_K_FRAC*100)}% Recall: {result['topk_recall']:.4f}\\n\\n\"\n",
    "            f\"Classification Report:\\n{result['cls_report']}\\n\"\n",
    "        )\n",
    "    log(f\"Saved test metrics -> {metrics_path}\")\n",
    "\n",
    "    # Save confusion matrix plot\n",
    "    cm_path = OUTPUT_DIR / \"confusion_matrix.png\"\n",
    "    plot_confusion_matrix(result[\"confusion_matrix\"], cm_path)\n",
    "    log(f\"Saved confusion matrix -> {cm_path}\")\n",
    "\n",
    "    # Save predictions (test)\n",
    "    pred_path = OUTPUT_DIR / \"predictions_test.csv\"\n",
    "    out_df = pd.DataFrame({\n",
    "        ID_COL: X_test[ID_COL].astype(str) if (ID_COL in X_test.columns) else np.arange(len(X_test)),\n",
    "        \"y_true\": y_test.values,\n",
    "        \"y_prob\": result[\"proba\"],\n",
    "        \"y_pred\": result[\"y_pred\"],\n",
    "    })\n",
    "    out_df.to_csv(pred_path, index=False)\n",
    "    log(f\"Saved test predictions -> {pred_path}\")\n",
    "\n",
    "    # Permutation importance\n",
    "    # Fit final pipeline on TRAIN (already fit inside evaluate_on_test), reuse it\n",
    "    pipe = result[\"pipeline\"]\n",
    "\n",
    "    # Build feature name list from fitted preprocessor\n",
    "    pre_fitted = pipe.named_steps[\"pre\"]\n",
    "    feature_names = get_feature_names(pre_fitted, num_cols, cat_cols)\n",
    "\n",
    "    imp_csv = OUTPUT_DIR / \"permutation_importance.csv\"\n",
    "    imp_png = OUTPUT_DIR / \"permutation_importance.png\"\n",
    "    plot_permutation_importance(pipe, X_test, y_test, feature_names, imp_csv, imp_png)\n",
    "    log(f\"Saved permutation importance -> {imp_csv}, {imp_png}\")\n",
    "\n",
    "    # Save model\n",
    "    if joblib is not None:\n",
    "        model_path = OUTPUT_DIR / \"support_needs_model.pkl\"\n",
    "        joblib.dump(pipe, model_path)\n",
    "        log(f\"Saved pipeline -> {model_path}\")\n",
    "    else:\n",
    "        log(\"joblib not available; skipping model serialization.\")\n",
    "\n",
    "    # Quick imbalance hint\n",
    "    pos_rate = y.mean()\n",
    "    if pos_rate < 0.1:\n",
    "        log(f\"NOTE: Positive rate is low ({pos_rate:.3f}). Consider alternative thresholds or cost-sensitive objectives.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "889f5109",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Loading: open/train.csv\n",
      "Numeric cols (6): ['age', 'tenure', 'frequent', 'payment_interval', 'contract_length', 'after_interaction']\n",
      "Categorical cols (2): ['gender', 'subscription_type']\n",
      "Models considered: ['logreg_multinomial', 'rf', 'gbdt', 'hist_gbdt']\n",
      "Saved CV results -> open/ml_outputs/cv_results.csv\n",
      "Best model: rf\n",
      "Saved test metrics -> open/ml_outputs/test_metrics.txt\n",
      "Saved confusion matrix -> open/ml_outputs/confusion_matrix.png\n",
      "Saved test predictions -> open/ml_outputs/predictions_test.csv\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "All arrays must be of the same length",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 419\u001b[39m\n\u001b[32m    415\u001b[39m         log(\u001b[33m\"\u001b[39m\u001b[33mjoblib not available; skipping model serialization.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    418\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m419\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 405\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    403\u001b[39m imp_csv = OUTPUT_DIR / \u001b[33m\"\u001b[39m\u001b[33mpermutation_importance.csv\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    404\u001b[39m imp_png = OUTPUT_DIR / \u001b[33m\"\u001b[39m\u001b[33mpermutation_importance.png\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m405\u001b[39m \u001b[43mplot_permutation_importance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpipe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimp_csv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimp_png\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    406\u001b[39m log(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mSaved permutation importance -> \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimp_csv\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimp_png\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    408\u001b[39m \u001b[38;5;66;03m# Save model\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 299\u001b[39m, in \u001b[36mplot_permutation_importance\u001b[39m\u001b[34m(pipe, X_test, y_test, feature_names, out_csv, out_png)\u001b[39m\n\u001b[32m    294\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mplot_permutation_importance\u001b[39m(pipe, X_test, y_test, feature_names, out_csv: Path, out_png: Path):\n\u001b[32m    295\u001b[39m     \u001b[38;5;66;03m# Use neg_log_loss to value probability quality across classes\u001b[39;00m\n\u001b[32m    296\u001b[39m     r = permutation_importance(\n\u001b[32m    297\u001b[39m         pipe, X_test, y_test, n_repeats=\u001b[32m10\u001b[39m, random_state=RANDOM_STATE, n_jobs=-\u001b[32m1\u001b[39m, scoring=\u001b[33m\"\u001b[39m\u001b[33mneg_log_loss\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    298\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m299\u001b[39m     imp = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfeature\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mimportance_mean\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mimportances_mean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mimportance_std\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mimportances_std\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    300\u001b[39m     imp.sort_values(\u001b[33m\"\u001b[39m\u001b[33mimportance_mean\u001b[39m\u001b[33m\"\u001b[39m, ascending=\u001b[38;5;28;01mFalse\u001b[39;00m, inplace=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    301\u001b[39m     imp.to_csv(out_csv, index=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git/python-playground/.venv/lib/python3.12/site-packages/pandas/core/frame.py:778\u001b[39m, in \u001b[36mDataFrame.__init__\u001b[39m\u001b[34m(self, data, index, columns, dtype, copy)\u001b[39m\n\u001b[32m    772\u001b[39m     mgr = \u001b[38;5;28mself\u001b[39m._init_mgr(\n\u001b[32m    773\u001b[39m         data, axes={\u001b[33m\"\u001b[39m\u001b[33mindex\u001b[39m\u001b[33m\"\u001b[39m: index, \u001b[33m\"\u001b[39m\u001b[33mcolumns\u001b[39m\u001b[33m\"\u001b[39m: columns}, dtype=dtype, copy=copy\n\u001b[32m    774\u001b[39m     )\n\u001b[32m    776\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mdict\u001b[39m):\n\u001b[32m    777\u001b[39m     \u001b[38;5;66;03m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m778\u001b[39m     mgr = \u001b[43mdict_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmanager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    779\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ma.MaskedArray):\n\u001b[32m    780\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mma\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m mrecords\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git/python-playground/.venv/lib/python3.12/site-packages/pandas/core/internals/construction.py:503\u001b[39m, in \u001b[36mdict_to_mgr\u001b[39m\u001b[34m(data, index, columns, dtype, typ, copy)\u001b[39m\n\u001b[32m    499\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    500\u001b[39m         \u001b[38;5;66;03m# dtype check to exclude e.g. range objects, scalars\u001b[39;00m\n\u001b[32m    501\u001b[39m         arrays = [x.copy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(x, \u001b[33m\"\u001b[39m\u001b[33mdtype\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m arrays]\n\u001b[32m--> \u001b[39m\u001b[32m503\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marrays_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtyp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconsolidate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git/python-playground/.venv/lib/python3.12/site-packages/pandas/core/internals/construction.py:114\u001b[39m, in \u001b[36marrays_to_mgr\u001b[39m\u001b[34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[39m\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m verify_integrity:\n\u001b[32m    112\u001b[39m     \u001b[38;5;66;03m# figure out the index, if necessary\u001b[39;00m\n\u001b[32m    113\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m         index = \u001b[43m_extract_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    116\u001b[39m         index = ensure_index(index)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git/python-playground/.venv/lib/python3.12/site-packages/pandas/core/internals/construction.py:677\u001b[39m, in \u001b[36m_extract_index\u001b[39m\u001b[34m(data)\u001b[39m\n\u001b[32m    675\u001b[39m lengths = \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mset\u001b[39m(raw_lengths))\n\u001b[32m    676\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(lengths) > \u001b[32m1\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m677\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mAll arrays must be of the same length\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    679\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m have_dicts:\n\u001b[32m    680\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    681\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mMixing dicts with non-Series may lead to ambiguous ordering.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    682\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: All arrays must be of the same length"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Support Needs Prediction (Multiclass) — No AutoML\n",
    "=================================================\n",
    "- Input columns (example):\n",
    "  ID, age, gender, tenure, frequent, payment_interval, subscription_type, contract_length, after_interaction, support_needs\n",
    "- Goal: Predict `support_needs` **(multiclass)** without AutoML frameworks.\n",
    "- What this script does:\n",
    "  1) Load & split data (stratified train/test)\n",
    "  2) Preprocess (impute, scale numeric; impute + one-hot categorical)\n",
    "  3) Train several fixed-parameter **multiclass** models (no hyperparameter search)\n",
    "  4) Cross-validate and pick the best model by **macro PR-friendly metrics** (e.g., F1-macro, log loss, ROC-AUC OVR macro)\n",
    "  5) (No binary threshold tuning) — use argmax of class probabilities\n",
    "  6) Evaluate on test (Accuracy, Balanced Acc, F1-macro, Precision/Recall-macro, LogLoss, ROC-AUC OVR macro)\n",
    "  7) Compute permutation feature importance and export artifacts\n",
    "  8) Save the fitted pipeline to disk (joblib)\n",
    "\n",
    "- Outputs (created under ./ml_outputs/):\n",
    "  - cv_results.csv, best_model.txt\n",
    "  - test_metrics.txt, confusion_matrix.png\n",
    "  - permutation_importance.csv, permutation_importance.png\n",
    "  - predictions_test.csv (ID, y_true, y_pred, proba_<class>...)\n",
    "  - support_needs_model.pkl (joblib pipeline)\n",
    "\n",
    "Notes\n",
    "-----\n",
    "- No AutoML libraries used; only fixed configurations and simple model selection.\n",
    "- Multiclass metrics emphasize **macro averages** to treat classes uniformly.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_validate\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    balanced_accuracy_score,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    log_loss,\n",
    "    roc_auc_score,\n",
    "    confusion_matrix,\n",
    "    classification_report,\n",
    "    ConfusionMatrixDisplay,\n",
    "    top_k_accuracy_score,\n",
    "    make_scorer,\n",
    ")\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, HistGradientBoostingClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "try:\n",
    "    import joblib\n",
    "except Exception:\n",
    "    joblib = None\n",
    "\n",
    "# =============================\n",
    "# Config\n",
    "# =============================\n",
    "CSV_PATH = Path(\"./open/train.csv\")\n",
    "TARGET_COL = \"support_needs\"\n",
    "ID_COL = \"ID\"\n",
    "RANDOM_STATE = 42\n",
    "TEST_SIZE = 0.2\n",
    "N_FOLDS = 5\n",
    "OUTPUT_DIR = CSV_PATH.parent / \"ml_outputs\"\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Columns expected (numeric/cat). We'll infer but can be overridden here.\n",
    "NUM_HINT = [\n",
    "    \"age\", \"tenure\", \"frequent\", \"payment_interval\",\n",
    "    \"contract_length\", \"after_interaction\"\n",
    "]\n",
    "CAT_HINT = [\"gender\", \"subscription_type\"]\n",
    "\n",
    "# =============================\n",
    "# Utility\n",
    "# =============================\n",
    "\n",
    "def log(msg: str):\n",
    "    print(msg)\n",
    "\n",
    "\n",
    "def read_data(csv_path: str) -> pd.DataFrame:\n",
    "    log(f\">> Loading: {csv_path}\")\n",
    "    df = pd.read_csv(csv_path)\n",
    "    return df\n",
    "\n",
    "\n",
    "def split_xy(df: pd.DataFrame):\n",
    "    assert TARGET_COL in df.columns, f\"Target column '{TARGET_COL}' not found.\"\n",
    "    y = df[TARGET_COL]  # keep labels as-is (can be strings)\n",
    "    X = df.drop(columns=[TARGET_COL])\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def infer_column_types(df: pd.DataFrame):\n",
    "    cols = df.columns.tolist()\n",
    "    cat_cols = []\n",
    "    num_cols = []\n",
    "    for c in cols:\n",
    "        if c == TARGET_COL or c == ID_COL:\n",
    "            continue\n",
    "        if c in NUM_HINT:\n",
    "            num_cols.append(c)\n",
    "        elif c in CAT_HINT:\n",
    "            cat_cols.append(c)\n",
    "            \n",
    "        else:\n",
    "            if pd.api.types.is_numeric_dtype(df[c]):\n",
    "                num_cols.append(c)\n",
    "            else:\n",
    "                sample = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "                if sample.notna().mean() > 0.9:\n",
    "                    num_cols.append(c)\n",
    "                else:\n",
    "                    cat_cols.append(c)\n",
    "    return num_cols, cat_cols\n",
    "\n",
    "\n",
    "def build_preprocessor(num_cols, cat_cols):\n",
    "    num_pipe = Pipeline(\n",
    "        steps=[\n",
    "            (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "            (\"scaler\", StandardScaler()),\n",
    "        ]\n",
    "    )\n",
    "    cat_pipe = Pipeline(\n",
    "        steps=[\n",
    "            (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "            (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\")),\n",
    "        ]\n",
    "    )\n",
    "    pre = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"num\", num_pipe, num_cols),\n",
    "            (\"cat\", cat_pipe, cat_cols),\n",
    "        ],\n",
    "        remainder=\"drop\",\n",
    "        verbose_feature_names_out=False,\n",
    "    )\n",
    "    return pre\n",
    "\n",
    "\n",
    "def get_models():\n",
    "    \"\"\"Return dict of fixed-parameter multiclass models (no HPO).\"\"\"\n",
    "    models = {\n",
    "        \"logreg_multinomial\": LogisticRegression(\n",
    "            max_iter=1000,\n",
    "            multi_class=\"multinomial\",\n",
    "            class_weight=\"balanced\",\n",
    "            solver=\"lbfgs\",\n",
    "            n_jobs=None if hasattr(LogisticRegression, 'n_jobs') else None,\n",
    "        ),\n",
    "        \"rf\": RandomForestClassifier(\n",
    "            n_estimators=400,\n",
    "            class_weight=\"balanced_subsample\",\n",
    "            random_state=RANDOM_STATE,\n",
    "            n_jobs=-1,\n",
    "        ),\n",
    "        \"gbdt\": GradientBoostingClassifier(random_state=RANDOM_STATE),\n",
    "        \"hist_gbdt\": HistGradientBoostingClassifier(\n",
    "            random_state=RANDOM_STATE,\n",
    "            max_depth=None,\n",
    "        ),\n",
    "    }\n",
    "    return models\n",
    "\n",
    "\n",
    "def build_scoring():\n",
    "    scoring = {\n",
    "        \"accuracy\": \"accuracy\",\n",
    "        \"balanced_accuracy\": \"balanced_accuracy\",\n",
    "        \"f1_macro\": \"f1_macro\",\n",
    "        \"precision_macro\": \"precision_macro\",\n",
    "        \"recall_macro\": \"recall_macro\",\n",
    "        \"neg_log_loss\": \"neg_log_loss\",\n",
    "        \"roc_auc_ovr_macro\": make_scorer(roc_auc_score, needs_proba=True, multi_class=\"ovr\", average=\"macro\"),\n",
    "        \"top3_accuracy\": make_scorer(top_k_accuracy_score, k=3),\n",
    "    }\n",
    "    return scoring\n",
    "\n",
    "\n",
    "def cross_validate_models(pre, models: dict, X_train: pd.DataFrame, y_train: pd.Series, n_folds: int = 5) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    cv = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=RANDOM_STATE)\n",
    "    scoring = build_scoring()\n",
    "    for name, clf in models.items():\n",
    "        pipe = Pipeline(steps=[(\"pre\", pre), (\"clf\", clf)])\n",
    "        scores = cross_validate(\n",
    "            pipe, X_train, y_train, cv=cv, scoring=scoring, n_jobs=-1, return_train_score=False\n",
    "        )\n",
    "        row = {\"model\": name}\n",
    "        for k, v in scores.items():\n",
    "            if k.startswith(\"test_\"):\n",
    "                m = k.replace(\"test_\", \"mean_\")\n",
    "                s = k.replace(\"test_\", \"std_\")\n",
    "                row[m] = np.mean(v)\n",
    "                row[s] = np.std(v)\n",
    "        rows.append(row)\n",
    "    df = pd.DataFrame(rows)\n",
    "    # Primary sort key: F1-macro, secondary: neg_log_loss (higher is better as it's neg), tertiary: roc_auc\n",
    "    df = df.sort_values([\"mean_f1_macro\", \"mean_neg_log_loss\", \"mean_roc_auc_ovr_macro\"], ascending=[False, False, False])\n",
    "    return df\n",
    "\n",
    "\n",
    "def pick_best_model(cv_df: pd.DataFrame) -> str:\n",
    "    best = cv_df.iloc[0]\n",
    "    return best[\"model\"]\n",
    "\n",
    "\n",
    "def evaluate_on_test(pre, clf, X_train, y_train, X_test, y_test):\n",
    "    pipe = Pipeline(steps=[(\"pre\", pre), (\"clf\", clf)])\n",
    "    pipe.fit(X_train, y_train)\n",
    "\n",
    "    proba = None\n",
    "    if hasattr(pipe, \"predict_proba\") or hasattr(pipe.named_steps[\"clf\"], \"predict_proba\"):\n",
    "        proba = pipe.predict_proba(X_test)\n",
    "    y_pred = pipe.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    bacc = balanced_accuracy_score(y_test, y_pred)\n",
    "    f1m = f1_score(y_test, y_pred, average=\"macro\")\n",
    "    precm = precision_score(y_test, y_pred, average=\"macro\", zero_division=0)\n",
    "    recm = recall_score(y_test, y_pred, average=\"macro\")\n",
    "\n",
    "    ll = None\n",
    "    roc_macro = None\n",
    "    if proba is not None:\n",
    "        ll = log_loss(y_test, proba)\n",
    "        try:\n",
    "            roc_macro = roc_auc_score(y_test, proba, multi_class=\"ovr\", average=\"macro\")\n",
    "        except Exception:\n",
    "            roc_macro = None\n",
    "\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    report = classification_report(y_test, y_pred, digits=4)\n",
    "\n",
    "    return {\n",
    "        \"pipeline\": pipe,\n",
    "        \"proba\": proba,\n",
    "        \"y_pred\": y_pred,\n",
    "        \"accuracy\": acc,\n",
    "        \"balanced_accuracy\": bacc,\n",
    "        \"f1_macro\": f1m,\n",
    "        \"precision_macro\": precm,\n",
    "        \"recall_macro\": recm,\n",
    "        \"log_loss\": ll,\n",
    "        \"roc_auc_ovr_macro\": roc_macro,\n",
    "        \"confusion_matrix\": cm,\n",
    "        \"cls_report\": report,\n",
    "    }\n",
    "\n",
    "\n",
    "def get_feature_names(pre, num_cols, cat_cols):\n",
    "    names = []\n",
    "    if len(num_cols) > 0:\n",
    "        names.extend(num_cols)\n",
    "    if len(cat_cols) > 0:\n",
    "        ohe = pre.named_transformers_[\"cat\"].named_steps[\"ohe\"]\n",
    "        try:\n",
    "            ohe_names = ohe.get_feature_names_out(cat_cols).tolist()\n",
    "        except Exception:\n",
    "            ohe_names = []\n",
    "            for i, c in enumerate(cat_cols):\n",
    "                cats = ohe.categories_[i]\n",
    "                ohe_names.extend([f\"{c}_{val}\" for val in cats])\n",
    "        names.extend(ohe_names)\n",
    "    return names\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(cm, class_labels, outpath: Path):\n",
    "    fig, ax = plt.subplots(figsize=(6, 5))\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_labels)\n",
    "    disp.plot(ax=ax, cmap=\"Blues\", values_format='d', colorbar=True)\n",
    "    ax.set_title(\"Confusion Matrix\")\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(outpath, dpi=140)\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "def plot_permutation_importance(pipe, X_test, y_test, feature_names, out_csv: Path, out_png: Path):\n",
    "    # Use neg_log_loss to value probability quality across classes\n",
    "    r = permutation_importance(\n",
    "        pipe, X_test, y_test, n_repeats=10, random_state=RANDOM_STATE, n_jobs=-1, scoring=\"neg_log_loss\"\n",
    "    )\n",
    "    imp = pd.DataFrame({\"feature\": feature_names, \"importance_mean\": r.importances_mean, \"importance_std\": r.importances_std})\n",
    "    imp.sort_values(\"importance_mean\", ascending=False, inplace=True)\n",
    "    imp.to_csv(out_csv, index=False)\n",
    "\n",
    "    top = imp.head(20)\n",
    "    fig, ax = plt.subplots(figsize=(8, 7))\n",
    "    ax.barh(top[\"feature\"][::-1], top[\"importance_mean\"][::-1])\n",
    "    ax.set_title(\"Permutation Importance (neg_log_loss)\")\n",
    "    ax.set_xlabel(\"Mean Importance\")\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(out_png, dpi=140)\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "def main():\n",
    "    df = read_data(CSV_PATH)\n",
    "\n",
    "    # Optional: keep ID aside\n",
    "    id_series = df[ID_COL].astype(str) if ID_COL in df.columns else None\n",
    "\n",
    "    X, y = split_xy(df)\n",
    "\n",
    "    # Infer column types\n",
    "    num_cols, cat_cols = infer_column_types(X)\n",
    "    log(f\"Numeric cols ({len(num_cols)}): {num_cols}\")\n",
    "    log(f\"Categorical cols ({len(cat_cols)}): {cat_cols}\")\n",
    "\n",
    "    # Train/test split (stratified)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=TEST_SIZE, stratify=y, random_state=RANDOM_STATE\n",
    "    )\n",
    "\n",
    "    # Preprocessor\n",
    "    pre = build_preprocessor(num_cols, cat_cols)\n",
    "\n",
    "    # Models\n",
    "    models = get_models()\n",
    "    log(f\"Models considered: {list(models.keys())}\")\n",
    "\n",
    "    # Cross-validate models on train\n",
    "    cv_df = cross_validate_models(pre, models, X_train, y_train, n_folds=N_FOLDS)\n",
    "    cv_path = OUTPUT_DIR / \"cv_results.csv\"\n",
    "    cv_df.to_csv(cv_path, index=False)\n",
    "    log(f\"Saved CV results -> {cv_path}\")\n",
    "\n",
    "    best_name = pick_best_model(cv_df)\n",
    "    best_clf = models[best_name]\n",
    "    with open(OUTPUT_DIR / \"best_model.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(f\"Best by F1-macro, neg_log_loss, ROC-AUC OVR macro: {best_name}\")\n",
    "        f.write(cv_df.head(1).to_string(index=False))\n",
    "    log(f\"Best model: {best_name}\")\n",
    "\n",
    "    # Final train and evaluate on test\n",
    "    result = evaluate_on_test(pre, best_clf, X_train, y_train, X_test, y_test)\n",
    "\n",
    "    # Save metrics\n",
    "    metrics_path = OUTPUT_DIR / \"test_metrics.txt\"\n",
    "    with open(metrics_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\n",
    "            (\n",
    "                f\"Accuracy: {result['accuracy']:.4f}\"\n",
    "                f\"Balanced Acc: {result['balanced_accuracy']:.4f}\"\n",
    "                f\"F1-macro: {result['f1_macro']:.4f}\"\n",
    "                f\"Precision-macro: {result['precision_macro']:.4f}\"\n",
    "                f\"Recall-macro: {result['recall_macro']:.4f}\"\n",
    "            )\n",
    "        )\n",
    "        if result[\"log_loss\"] is not None:\n",
    "            f.write(f\"LogLoss: {result['log_loss']:.4f}\")\n",
    "        if result[\"roc_auc_ovr_macro\"] is not None:\n",
    "            f.write(f\"ROC-AUC (OVR macro): {result['roc_auc_ovr_macro']:.4f}\")\n",
    "        f.write(\"Classification Report:\")\n",
    "        f.write(result[\"cls_report\"])    \n",
    "    log(f\"Saved test metrics -> {metrics_path}\")\n",
    "\n",
    "    # Save confusion matrix plot\n",
    "    classes = None\n",
    "    # get class labels from fitted estimator\n",
    "    try:\n",
    "        classes = result[\"pipeline\"].named_steps[\"clf\"].classes_\n",
    "    except Exception:\n",
    "        classes = np.unique(y)\n",
    "\n",
    "    cm_path = OUTPUT_DIR / \"confusion_matrix.png\"\n",
    "    plot_confusion_matrix(result[\"confusion_matrix\"], classes, cm_path)\n",
    "    log(f\"Saved confusion matrix -> {cm_path}\")\n",
    "\n",
    "    # Save predictions (test), including per-class probabilities if available\n",
    "    pred_path = OUTPUT_DIR / \"predictions_test.csv\"\n",
    "    out_df = pd.DataFrame({\n",
    "        ID_COL: X_test[ID_COL].astype(str) if (ID_COL in X_test.columns) else np.arange(len(X_test)),\n",
    "        \"y_true\": y_test.values,\n",
    "        \"y_pred\": result[\"y_pred\"],\n",
    "    })\n",
    "    if result[\"proba\"] is not None:\n",
    "        for i, c in enumerate(classes):\n",
    "            out_df[f\"proba_{c}\"] = result[\"proba\"][:, i]\n",
    "    out_df.to_csv(pred_path, index=False)\n",
    "    log(f\"Saved test predictions -> {pred_path}\")\n",
    "\n",
    "    # Permutation importance (uses neg_log_loss scorer)\n",
    "    pipe = result[\"pipeline\"]\n",
    "    pre_fitted = pipe.named_steps[\"pre\"]\n",
    "    feature_names = get_feature_names(pre_fitted, num_cols, cat_cols)\n",
    "    imp_csv = OUTPUT_DIR / \"permutation_importance.csv\"\n",
    "    imp_png = OUTPUT_DIR / \"permutation_importance.png\"\n",
    "    plot_permutation_importance(pipe, X_test, y_test, feature_names, imp_csv, imp_png)\n",
    "    log(f\"Saved permutation importance -> {imp_csv}, {imp_png}\")\n",
    "\n",
    "    # Save model\n",
    "    if joblib is not None:\n",
    "        model_path = OUTPUT_DIR / \"support_needs_model.pkl\"\n",
    "        from joblib import dump\n",
    "        dump(pipe, model_path)\n",
    "        log(f\"Saved pipeline -> {model_path}\")\n",
    "    else:\n",
    "        log(\"joblib not available; skipping model serialization.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e35a4466",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Loading: open/train.csv\n",
      "Numeric cols (6): ['age', 'tenure', 'frequent', 'payment_interval', 'contract_length', 'after_interaction']\n",
      "Categorical cols (2): ['gender', 'subscription_type']\n",
      "Models considered: ['logreg_multinomial', 'rf', 'gbdt', 'hist_gbdt', 'xgb', 'lgbm']\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000915 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001046 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001077 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 218\n",
      "[LightGBM] [Info] Total Bins 218\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000591 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 218\n",
      "[LightGBM] [Info] Total Bins 218\n",
      "[LightGBM] [Info] Number of data points in the train set: 19749, number of used features: 11[LightGBM] [Info] Number of data points in the train set: 19749, number of used features: 11\n",
      "\n",
      "[LightGBM] [Info] Number of data points in the train set: 19748, number of used features: 11\n",
      "[LightGBM] [Info] Number of data points in the train set: 19749, number of used features: 11\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001060 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.[LightGBM] [Info] Start training from score -1.098612\n",
      "\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Total Bins 218\n",
      "[LightGBM] [Info] Number of data points in the train set: 19749, number of used features: 11\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "Saved CV results -> open/ml_outputs/cv_results.csv\n",
      "Best model: lgbm\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000174 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 218\n",
      "[LightGBM] [Info] Number of data points in the train set: 24686, number of used features: 11\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "Saved test metrics -> open/ml_outputs/test_metrics.txt\n",
      "Saved confusion matrix -> open/ml_outputs/confusion_matrix.png\n",
      "Saved test predictions -> open/ml_outputs/predictions_test.csv\n",
      "Saved permutation importance -> open/ml_outputs/permutation_importance.csv, open/ml_outputs/permutation_importance.png\n",
      "Saved pipeline -> open/ml_outputs/support_needs_model.pkl\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Support Needs Prediction (Multiclass) — No AutoML\n",
    "=================================================\n",
    "- Input columns (example):\n",
    "  ID, age, gender, tenure, frequent, payment_interval, subscription_type, contract_length, after_interaction, support_needs\n",
    "- Goal: Predict `support_needs` **(multiclass)** without AutoML frameworks.\n",
    "- What this script does:\n",
    "  1) Load & split data (stratified train/test)\n",
    "  2) Preprocess (impute, scale numeric; impute + one-hot categorical)\n",
    "  3) Train several fixed-parameter **multiclass** models (no hyperparameter search)\n",
    "  4) Cross-validate and pick the best model by **macro PR-friendly metrics** (e.g., F1-macro, log loss, ROC-AUC OVR macro)\n",
    "  5) (No binary threshold tuning) — use argmax of class probabilities\n",
    "  6) Evaluate on test (Accuracy, Balanced Acc, F1-macro, Precision/Recall-macro, LogLoss, ROC-AUC OVR macro)\n",
    "  7) Compute permutation feature importance and export artifacts\n",
    "  8) Save the fitted pipeline to disk (joblib)\n",
    "\n",
    "- Outputs (created under ./ml_outputs/):\n",
    "  - cv_results.csv, best_model.txt\n",
    "  - test_metrics.txt, confusion_matrix.png\n",
    "  - permutation_importance.csv, permutation_importance.png\n",
    "  - predictions_test.csv (ID, y_true, y_pred, proba_<class>...)\n",
    "  - support_needs_model.pkl (joblib pipeline)\n",
    "\n",
    "Notes\n",
    "-----\n",
    "- No AutoML libraries used; only fixed configurations and simple model selection.\n",
    "- Multiclass metrics emphasize **macro averages** to treat classes uniformly.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_validate\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    balanced_accuracy_score,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    log_loss,\n",
    "    roc_auc_score,\n",
    "    confusion_matrix,\n",
    "    classification_report,\n",
    "    ConfusionMatrixDisplay,\n",
    "    top_k_accuracy_score,\n",
    "    make_scorer,\n",
    ")\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, HistGradientBoostingClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "try:\n",
    "    import joblib\n",
    "except Exception:\n",
    "    joblib = None\n",
    "\n",
    "# =============================\n",
    "# Config\n",
    "# =============================\n",
    "CSV_PATH = Path(\"./open/train.csv\")\n",
    "TARGET_COL = \"support_needs\"\n",
    "ID_COL = \"ID\"\n",
    "RANDOM_STATE = 42\n",
    "TEST_SIZE = 0.2\n",
    "N_FOLDS = 5\n",
    "OUTPUT_DIR = CSV_PATH.parent / \"ml_outputs\"\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Columns expected (numeric/cat). We'll infer but can be overridden here.\n",
    "NUM_HINT = [\n",
    "    \"age\", \"tenure\", \"frequent\", \"payment_interval\",\n",
    "    \"contract_length\", \"after_interaction\"\n",
    "]\n",
    "CAT_HINT = [\"gender\", \"subscription_type\"]\n",
    "\n",
    "# =============================\n",
    "# Utility\n",
    "# =============================\n",
    "\n",
    "def log(msg: str):\n",
    "    print(msg)\n",
    "\n",
    "\n",
    "def read_data(csv_path: str) -> pd.DataFrame:\n",
    "    log(f\">> Loading: {csv_path}\")\n",
    "    df = pd.read_csv(csv_path)\n",
    "    return df\n",
    "\n",
    "\n",
    "def split_xy(df: pd.DataFrame):\n",
    "    assert TARGET_COL in df.columns, f\"Target column '{TARGET_COL}' not found.\"\n",
    "    y = df[TARGET_COL]  # keep labels as-is (can be strings)\n",
    "    X = df.drop(columns=[TARGET_COL])\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def infer_column_types(df: pd.DataFrame):\n",
    "    cols = df.columns.tolist()\n",
    "    cat_cols = []\n",
    "    num_cols = []\n",
    "    for c in cols:\n",
    "        if c == TARGET_COL or c == ID_COL:\n",
    "            continue\n",
    "        if c in NUM_HINT:\n",
    "            num_cols.append(c)\n",
    "        elif c in CAT_HINT:\n",
    "            cat_cols.append(c)\n",
    "            \n",
    "        else:\n",
    "            if pd.api.types.is_numeric_dtype(df[c]):\n",
    "                num_cols.append(c)\n",
    "            else:\n",
    "                sample = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "                if sample.notna().mean() > 0.9:\n",
    "                    num_cols.append(c)\n",
    "                else:\n",
    "                    cat_cols.append(c)\n",
    "    return num_cols, cat_cols\n",
    "\n",
    "\n",
    "def build_preprocessor(num_cols, cat_cols):\n",
    "    num_pipe = Pipeline(\n",
    "        steps=[\n",
    "            (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "            (\"scaler\", StandardScaler()),\n",
    "        ]\n",
    "    )\n",
    "    cat_pipe = Pipeline(\n",
    "        steps=[\n",
    "            (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "            (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\")),\n",
    "        ]\n",
    "    )\n",
    "    pre = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"num\", num_pipe, num_cols),\n",
    "            (\"cat\", cat_pipe, cat_cols),\n",
    "        ],\n",
    "        remainder=\"drop\",\n",
    "        verbose_feature_names_out=False,\n",
    "    )\n",
    "    return pre\n",
    "\n",
    "\n",
    "def get_models(y_train: pd.Series):\n",
    "    \"\"\"Return dict of fixed-parameter multiclass models (no HPO).\"\"\"\n",
    "\n",
    "    pos_rate = y_train.mean()\n",
    "    neg_rate = 1 - pos_rate\n",
    "    scale_pos_weight = (neg_rate / pos_rate) if pos_rate > 0 else 1.0\n",
    "    models = {\n",
    "        \"logreg_multinomial\": LogisticRegression(\n",
    "            max_iter=1000,\n",
    "            multi_class=\"multinomial\",\n",
    "            class_weight=\"balanced\",\n",
    "            solver=\"lbfgs\",\n",
    "            n_jobs=None if hasattr(LogisticRegression, 'n_jobs') else None,\n",
    "        ),\n",
    "        \"rf\": RandomForestClassifier(\n",
    "            n_estimators=400,\n",
    "            class_weight=\"balanced_subsample\",\n",
    "            random_state=RANDOM_STATE,\n",
    "            n_jobs=-1,\n",
    "        ),\n",
    "        \"gbdt\": GradientBoostingClassifier(random_state=RANDOM_STATE),\n",
    "        \"hist_gbdt\": HistGradientBoostingClassifier(\n",
    "            random_state=RANDOM_STATE,\n",
    "            max_depth=None,\n",
    "        ),\n",
    "    }\n",
    "\n",
    "        # Optional: XGBoost\n",
    "    try:\n",
    "        import xgboost as xgb\n",
    "        models[\"xgb\"] = xgb.XGBClassifier(\n",
    "            n_estimators=500,\n",
    "            max_depth=5,\n",
    "            learning_rate=0.05,\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.8,\n",
    "            reg_lambda=1.0,\n",
    "            random_state=RANDOM_STATE,\n",
    "            tree_method=\"hist\",\n",
    "            eval_metric=\"logloss\",\n",
    "            n_jobs=-1,\n",
    "            scale_pos_weight=scale_pos_weight,\n",
    "        )\n",
    "    except Exception as e:\n",
    "        log(e)\n",
    "        pass\n",
    "\n",
    "    # Optional: LightGBM\n",
    "    try:\n",
    "        import lightgbm as lgb\n",
    "        models[\"lgbm\"] = lgb.LGBMClassifier(\n",
    "            n_estimators=800,\n",
    "            num_leaves=63,\n",
    "            learning_rate=0.03,\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.8,\n",
    "            reg_lambda=1.0,\n",
    "            random_state=RANDOM_STATE,\n",
    "            class_weight=\"balanced\",\n",
    "            n_jobs=-1,\n",
    "        )\n",
    "    except Exception as e:\n",
    "        log(e)\n",
    "        pass\n",
    "    \n",
    "    return models\n",
    "\n",
    "\n",
    "def build_scoring():\n",
    "    scoring = {\n",
    "        \"accuracy\": \"accuracy\",\n",
    "        \"balanced_accuracy\": \"balanced_accuracy\",\n",
    "        \"f1_macro\": \"f1_macro\",\n",
    "        \"precision_macro\": \"precision_macro\",\n",
    "        \"recall_macro\": \"recall_macro\",\n",
    "        \"neg_log_loss\": \"neg_log_loss\",\n",
    "        \"roc_auc_ovr_macro\": make_scorer(roc_auc_score, needs_proba=True, multi_class=\"ovr\", average=\"macro\"),\n",
    "        \"top3_accuracy\": make_scorer(top_k_accuracy_score, k=3),\n",
    "    }\n",
    "    return scoring\n",
    "\n",
    "\n",
    "def cross_validate_models(pre, models: dict, X_train: pd.DataFrame, y_train: pd.Series, n_folds: int = 5) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    cv = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=RANDOM_STATE)\n",
    "    scoring = build_scoring()\n",
    "    for name, clf in models.items():\n",
    "        pipe = Pipeline(steps=[(\"pre\", pre), (\"clf\", clf)])\n",
    "        scores = cross_validate(\n",
    "            pipe, X_train, y_train, cv=cv, scoring=scoring, n_jobs=-1, return_train_score=False\n",
    "        )\n",
    "        row = {\"model\": name}\n",
    "        for k, v in scores.items():\n",
    "            if k.startswith(\"test_\"):\n",
    "                m = k.replace(\"test_\", \"mean_\")\n",
    "                s = k.replace(\"test_\", \"std_\")\n",
    "                row[m] = np.mean(v)\n",
    "                row[s] = np.std(v)\n",
    "        rows.append(row)\n",
    "    df = pd.DataFrame(rows)\n",
    "    # Primary sort key: F1-macro, secondary: neg_log_loss (higher is better as it's neg), tertiary: roc_auc\n",
    "    df = df.sort_values([\"mean_f1_macro\", \"mean_neg_log_loss\", \"mean_roc_auc_ovr_macro\"], ascending=[False, False, False])\n",
    "    return df\n",
    "\n",
    "\n",
    "def pick_best_model(cv_df: pd.DataFrame) -> str:\n",
    "    best = cv_df.iloc[0]\n",
    "    return best[\"model\"]\n",
    "\n",
    "\n",
    "def evaluate_on_test(pre, clf, X_train, y_train, X_test, y_test):\n",
    "    pipe = Pipeline(steps=[(\"pre\", pre), (\"clf\", clf)])\n",
    "    pipe.fit(X_train, y_train)\n",
    "\n",
    "    proba = None\n",
    "    if hasattr(pipe, \"predict_proba\") or hasattr(pipe.named_steps[\"clf\"], \"predict_proba\"):\n",
    "        proba = pipe.predict_proba(X_test)\n",
    "    y_pred = pipe.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    bacc = balanced_accuracy_score(y_test, y_pred)\n",
    "    f1m = f1_score(y_test, y_pred, average=\"macro\")\n",
    "    precm = precision_score(y_test, y_pred, average=\"macro\", zero_division=0)\n",
    "    recm = recall_score(y_test, y_pred, average=\"macro\")\n",
    "\n",
    "    ll = None\n",
    "    roc_macro = None\n",
    "    if proba is not None:\n",
    "        ll = log_loss(y_test, proba)\n",
    "        try:\n",
    "            roc_macro = roc_auc_score(y_test, proba, multi_class=\"ovr\", average=\"macro\")\n",
    "        except Exception:\n",
    "            roc_macro = None\n",
    "\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    report = classification_report(y_test, y_pred, digits=4)\n",
    "\n",
    "    return {\n",
    "        \"pipeline\": pipe,\n",
    "        \"proba\": proba,\n",
    "        \"y_pred\": y_pred,\n",
    "        \"accuracy\": acc,\n",
    "        \"balanced_accuracy\": bacc,\n",
    "        \"f1_macro\": f1m,\n",
    "        \"precision_macro\": precm,\n",
    "        \"recall_macro\": recm,\n",
    "        \"log_loss\": ll,\n",
    "        \"roc_auc_ovr_macro\": roc_macro,\n",
    "        \"confusion_matrix\": cm,\n",
    "        \"cls_report\": report,\n",
    "    }\n",
    "\n",
    "\n",
    "def get_feature_names(pre, num_cols, cat_cols):\n",
    "    names = []\n",
    "    if len(num_cols) > 0:\n",
    "        names.extend(num_cols)\n",
    "    if len(cat_cols) > 0:\n",
    "        ohe = pre.named_transformers_[\"cat\"].named_steps[\"ohe\"]\n",
    "        try:\n",
    "            ohe_names = ohe.get_feature_names_out(cat_cols).tolist()\n",
    "        except Exception:\n",
    "            ohe_names = []\n",
    "            for i, c in enumerate(cat_cols):\n",
    "                cats = ohe.categories_[i]\n",
    "                ohe_names.extend([f\"{c}_{val}\" for val in cats])\n",
    "        names.extend(ohe_names)\n",
    "    return names\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(cm, class_labels, outpath: Path):\n",
    "    fig, ax = plt.subplots(figsize=(6, 5))\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_labels)\n",
    "    disp.plot(ax=ax, cmap=\"Blues\", values_format='d', colorbar=True)\n",
    "    ax.set_title(\"Confusion Matrix\")\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(outpath, dpi=140)\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "def plot_permutation_importance(pipe, X_test, y_test, feature_names, out_csv: Path, out_png: Path):\n",
    "    \"\"\"\n",
    "    Compute permutation importance **after preprocessing** so the number of\n",
    "    features matches the expanded (OHE) feature space. This avoids length\n",
    "    mismatches between `feature_names` (from the fitted preprocessor) and the\n",
    "    importances returned by sklearn.\n",
    "    \"\"\"\n",
    "    pre = pipe.named_steps[\"pre\"]\n",
    "    clf = pipe.named_steps[\"clf\"]\n",
    "\n",
    "    # Transform input into model space (after ColumnTransformer + OHE + scaling)\n",
    "    Xt = pre.transform(X_test)\n",
    "\n",
    "    r = permutation_importance(\n",
    "        clf, Xt, y_test,\n",
    "        n_repeats=10,\n",
    "        random_state=RANDOM_STATE,\n",
    "        n_jobs=-1,\n",
    "        scoring=\"neg_log_loss\",\n",
    "    )\n",
    "\n",
    "    # Safety: if lengths still mismatch, fall back to index names\n",
    "    if len(feature_names) != r.importances_mean.shape[0]:\n",
    "        feature_names = [f\"f{i}\" for i in range(r.importances_mean.shape[0])]\n",
    "\n",
    "    imp = pd.DataFrame({\n",
    "        \"feature\": feature_names,\n",
    "        \"importance_mean\": r.importances_mean,\n",
    "        \"importance_std\": r.importances_std,\n",
    "    })\n",
    "    imp.sort_values(\"importance_mean\", ascending=False, inplace=True)\n",
    "    imp.to_csv(out_csv, index=False)\n",
    "\n",
    "    top = imp.head(20)\n",
    "    fig, ax = plt.subplots(figsize=(8, 7))\n",
    "    ax.barh(top[\"feature\"][::-1], top[\"importance_mean\"][::-1])\n",
    "    ax.set_title(\"Permutation Importance (neg_log_loss)\")\n",
    "    ax.set_xlabel(\"Mean Importance\")\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(out_png, dpi=140)\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "def main():\n",
    "    df = read_data(CSV_PATH)\n",
    "\n",
    "    # Optional: keep ID aside\n",
    "    id_series = df[ID_COL].astype(str) if ID_COL in df.columns else None\n",
    "\n",
    "    X, y = split_xy(df)\n",
    "\n",
    "    # Infer column types\n",
    "    num_cols, cat_cols = infer_column_types(X)\n",
    "    log(f\"Numeric cols ({len(num_cols)}): {num_cols}\")\n",
    "    log(f\"Categorical cols ({len(cat_cols)}): {cat_cols}\")\n",
    "\n",
    "    # Train/test split (stratified)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=TEST_SIZE, stratify=y, random_state=RANDOM_STATE\n",
    "    )\n",
    "\n",
    "    # Preprocessor\n",
    "    pre = build_preprocessor(num_cols, cat_cols)\n",
    "\n",
    "    # Models\n",
    "    models = get_models(y_train)\n",
    "    log(f\"Models considered: {list(models.keys())}\")\n",
    "\n",
    "    # Cross-validate models on train\n",
    "    cv_df = cross_validate_models(pre, models, X_train, y_train, n_folds=N_FOLDS)\n",
    "    cv_path = OUTPUT_DIR / \"cv_results.csv\"\n",
    "    cv_df.to_csv(cv_path, index=False)\n",
    "    log(f\"Saved CV results -> {cv_path}\")\n",
    "\n",
    "    best_name = pick_best_model(cv_df)\n",
    "    best_clf = models[best_name]\n",
    "    with open(OUTPUT_DIR / \"best_model.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(f\"Best by F1-macro, neg_log_loss, ROC-AUC OVR macro: {best_name}\")\n",
    "        f.write(cv_df.head(1).to_string(index=False))\n",
    "    log(f\"Best model: {best_name}\")\n",
    "\n",
    "    # Final train and evaluate on test\n",
    "    result = evaluate_on_test(pre, best_clf, X_train, y_train, X_test, y_test)\n",
    "\n",
    "    # Save metrics\n",
    "    metrics_path = OUTPUT_DIR / \"test_metrics.txt\"\n",
    "    with open(metrics_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\n",
    "            (\n",
    "                f\"Accuracy: {result['accuracy']:.4f}\"\n",
    "                f\"Balanced Acc: {result['balanced_accuracy']:.4f}\"\n",
    "                f\"F1-macro: {result['f1_macro']:.4f}\"\n",
    "                f\"Precision-macro: {result['precision_macro']:.4f}\"\n",
    "                f\"Recall-macro: {result['recall_macro']:.4f}\"\n",
    "            )\n",
    "        )\n",
    "        if result[\"log_loss\"] is not None:\n",
    "            f.write(f\"LogLoss: {result['log_loss']:.4f}\")\n",
    "        if result[\"roc_auc_ovr_macro\"] is not None:\n",
    "            f.write(f\"ROC-AUC (OVR macro): {result['roc_auc_ovr_macro']:.4f}\")\n",
    "        f.write(\"Classification Report:\")\n",
    "        f.write(result[\"cls_report\"])    \n",
    "    log(f\"Saved test metrics -> {metrics_path}\")\n",
    "\n",
    "    # Save confusion matrix plot\n",
    "    classes = None\n",
    "    # get class labels from fitted estimator\n",
    "    try:\n",
    "        classes = result[\"pipeline\"].named_steps[\"clf\"].classes_\n",
    "    except Exception:\n",
    "        classes = np.unique(y)\n",
    "\n",
    "    cm_path = OUTPUT_DIR / \"confusion_matrix.png\"\n",
    "    plot_confusion_matrix(result[\"confusion_matrix\"], classes, cm_path)\n",
    "    log(f\"Saved confusion matrix -> {cm_path}\")\n",
    "\n",
    "    # Save predictions (test), including per-class probabilities if available\n",
    "    pred_path = OUTPUT_DIR / \"predictions_test.csv\"\n",
    "    out_df = pd.DataFrame({\n",
    "        ID_COL: X_test[ID_COL].astype(str) if (ID_COL in X_test.columns) else np.arange(len(X_test)),\n",
    "        \"y_true\": y_test.values,\n",
    "        \"y_pred\": result[\"y_pred\"],\n",
    "    })\n",
    "    if result[\"proba\"] is not None:\n",
    "        for i, c in enumerate(classes):\n",
    "            out_df[f\"proba_{c}\"] = result[\"proba\"][:, i]\n",
    "    out_df.to_csv(pred_path, index=False)\n",
    "    log(f\"Saved test predictions -> {pred_path}\")\n",
    "\n",
    "    # Permutation importance (uses neg_log_loss scorer)\n",
    "    pipe = result[\"pipeline\"]\n",
    "    pre_fitted = pipe.named_steps[\"pre\"]\n",
    "    feature_names = get_feature_names(pre_fitted, num_cols, cat_cols)\n",
    "    imp_csv = OUTPUT_DIR / \"permutation_importance.csv\"\n",
    "    imp_png = OUTPUT_DIR / \"permutation_importance.png\"\n",
    "    plot_permutation_importance(pipe, X_test, y_test, feature_names, imp_csv, imp_png)\n",
    "    log(f\"Saved permutation importance -> {imp_csv}, {imp_png}\")\n",
    "\n",
    "    # Save model\n",
    "    if joblib is not None:\n",
    "        model_path = OUTPUT_DIR / \"support_needs_model.pkl\"\n",
    "        from joblib import dump\n",
    "        dump(pipe, model_path)\n",
    "        log(f\"Saved pipeline -> {model_path}\")\n",
    "    else:\n",
    "        log(\"joblib not available; skipping model serialization.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e3c059",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, json, joblib\n",
    "\n",
    "\n",
    "file_path = OUTPUT_DIR / \"support_needs_model.pkl\"\n",
    "pipe = joblib.load(\"ml_outputs/support_needs_model.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f0a651db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('open/test.csv')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CSV_PATH.parent / \"test.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b2660624",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved -> /Users/jeongho/git/python-playground/customer_tier_classification/open/ml_outputs/predictions_real_test.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from pathlib import Path\n",
    "\n",
    "MODEL_PATH = OUTPUT_DIR / \"support_needs_model.pkl\"\n",
    "TEST_CSV  = CSV_PATH.parent / \"test.csv\"\n",
    "OUT_PATH  = OUTPUT_DIR / \"predictions_real_test.csv\"\n",
    "ID_COL    = \"ID\"\n",
    "TARGET_COL = \"support_needs\"  # 실제 테스트에 라벨이 없다면 자동으로 무시됨\n",
    "\n",
    "# 1) 모델 로드\n",
    "pipe = joblib.load(MODEL_PATH)\n",
    "\n",
    "# 2) 테스트 로드\n",
    "df_test = pd.read_csv(TEST_CSV)\n",
    "\n",
    "# (옵션) 필요한 입력 컬럼 확인 & 누락 시 NaN으로 채워 전처리기에 맡기기\n",
    "pre = pipe.named_steps[\"pre\"]\n",
    "required_cols = []\n",
    "for name, trans, cols in pre.transformers:\n",
    "    if name in (\"num\", \"cat\"):\n",
    "        required_cols += list(cols)\n",
    "\n",
    "missing = [c for c in required_cols if c not in df_test.columns]\n",
    "for c in missing:\n",
    "    df_test[c] = np.nan  # SimpleImputer가 처리\n",
    "\n",
    "# 3) 예측\n",
    "y_pred = pipe.predict(df_test)\n",
    "proba = None\n",
    "if hasattr(pipe.named_steps[\"clf\"], \"predict_proba\"):\n",
    "    proba = pipe.predict_proba(df_test)\n",
    "classes = pipe.named_steps[\"clf\"].classes_\n",
    "\n",
    "# 4) 저장 (ID 포함)\n",
    "out = pd.DataFrame({\n",
    "    ID_COL: df_test[ID_COL].astype(str) if ID_COL in df_test.columns else np.arange(len(df_test)),\n",
    "    \"support_needs\": y_pred\n",
    "})\n",
    "# if proba is not None:\n",
    "#     for i, c in enumerate(classes):\n",
    "#         out[f\"proba_{c}\"] = proba[:, i]\n",
    "\n",
    "# (옵션) 테스트에 정답 라벨이 있으면 간단 평가도 출력\n",
    "if TARGET_COL in df_test.columns:\n",
    "    from sklearn.metrics import classification_report\n",
    "    print(classification_report(df_test[TARGET_COL], y_pred, digits=4))\n",
    "\n",
    "OUT_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "out.to_csv(OUT_PATH, index=False)\n",
    "print(f\"Saved -> {OUT_PATH.resolve()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ab9943",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python-playground (3.12.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
